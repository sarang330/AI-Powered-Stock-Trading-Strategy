# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12SS0Mfvp5ENjJRzzZ8llKFBffTO7yi0Y
"""



# To run this script:
# 1. Save it as a Python file (e.g., streamlit_stock_app.py).
# 2. Open your terminal or command prompt.
# 3. Navigate to the directory where you saved the file.
# 4. Run the command: streamlit run streamlit_stock_app.py

# Note: You might need to install Streamlit first: pip install streamlit

# INSTALL DEPENDENCIES (Run this in your terminal if you haven't already)
# pip install streamlit yfinance xgboost lightgbm scikit-learn plotly pandas numpy imbalanced-learn shap

import streamlit as st
import os, warnings, sys
import numpy as np, pandas as pd
from datetime import datetime, timedelta
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from imblearn.over_sampling import RandomOverSampler
import xgboost as xgb
import lightgbm as lgb
import plotly.graph_objects as go
import shap
import matplotlib.pyplot as plt # Import matplotlib for SHAP plots

# ---- Quiet noisy libs and warnings ----
warnings.filterwarnings("ignore")
os.environ["PYTHONWARNINGS"] = "ignore"
# suppress LightGBM/XGBoost verbose options if any

# ---------- Global Parameters ----------
# Define RANDOM_STATE here to ensure it's available globally
RANDOM_STATE = 42

# ---------- Helper Functions (from previous analysis script) ----------

# Color helpers for terminal (not directly used in Streamlit, but kept for completeness)
class C:
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    BLUE = "\033[94m"
    ENDC = "\033[0m"

def color_text(text, color):
    return f"{color}{text}{C.ENDC}"

# Function to get top Nifty 500 tickers (static list for simplicity)
# In a real application, you might fetch this dynamically or from a reliable source
def get_nifty500_tickers():
    # This is a static example list. A dynamic approach would involve scraping
    # or using an API (potentially requiring a paid service) to get the current list.
    # This list contains a mix of known tickers to demonstrate the multi-select.
    # It's not the actual top 500 and may need updating or replacing.
    example_tickers = [
        "RELIANCE", "TCS", "HDFCBANK", "INFY", "ICICIBANK", "HINDUNILVR",
        "SBIN", "BHARTIARTL", "ITC", "LT", "BAJFINANCE", "ASIANPAINT",
        "KOTAKBANK", "MARUTI", "HCLTECH", "AXISBANK", "SUNPHARMA", "TITAN",
        "ULTRACEMCO", "NESTLEIND", "POWERGRID", "TECHM", "NTPC", "ADANIENT",
        "GRASIM", "ONGC", "TATAMOTORS", "JSWSTEEL", "SHREECEM", "INDUSINDBK",
        "WIPRO", "M&M", "BAJAJFINSV", "HDFC", "BRITANNIA", "DIVISLAB",
        "CIPLA", "COALINDIA", "DRREDDY", "EICHERMOT", "HEROMOTOCO", "HINDALCO",
        "IOC", "INFRAIT", "MCDOWELL-N", "MUTHOOTFIN", "PIIND", "PIDILITIND",
        "PEL", "SBILIFE", "SIEMENS", "SRF", "TATACHEM", "TATACONSUM",
        "TATASTEEL", "TCS", "TECHM", "TITAN", "ULTRACEMCO", "UPL", "VEDL",
        "WIPRO", "ZEEL"
        # Add more tickers here to reach closer to 500 if needed, or
        # replace this list with data from a reliable source.
    ]
    return sorted(list(set(example_tickers))) # Return unique sorted tickers

# ---------- Fetch history ----------
@st.cache_data # Cache data fetching
def fetch_history_yf(ticker, start, end):
    sym = ticker + ".NS"
    # st.write(f"Fetching {sym} from {start} to {end} via yfinance ...") # Use st.write for Streamlit output
    try:
        # Use auto_adjust=True to automatically get adjusted close prices
        df = yf.download(sym, start=start, end=end, progress=False, auto_adjust=True)
        if df.empty:
            st.warning(f"⚠ Warning: No historical data returned for {ticker}. Check ticker symbol or date range.")
            return pd.DataFrame() # Return empty DataFrame on failure
        df = df.reset_index().rename(columns={'Date':'Date'})
        df['Date'] = pd.to_datetime(df['Date'])
        # With auto_adjust=True, 'Adj Close' is not needed as 'Close' is already adjusted
        df = df[['Date','Open','High','Low','Close','Volume']]
        df.columns = ['Date','Open','High','Low','Close','Volume']
        return df
    except Exception as e:
        st.error(f"Error fetching data for {ticker}: {e}")
        return pd.DataFrame()


# ---------- Indicators ----------
def add_indicators(df):
    d = df.copy().sort_values('Date').reset_index(drop=True)
    d['ret1'] = d['Close'].pct_change()
    d['SMA50'] = d['Close'].rolling(50, min_periods=1).mean()
    d['SMA200'] = d['Close'].rolling(200, min_periods=1).mean()
    d['EMA20'] = d['Close'].ewm(span=20, adjust=False).mean()
    d['EMA50'] = d['Close'].ewm(span=50, adjust=False).mean()
    d['EMA12'] = d['Close'].ewm(span=12, adjust=False).mean()
    d['EMA26'] = d['Close'].ewm(span=26, adjust=False).mean()
    d['MACD'] = d['EMA12'] - d['EMA26']
    # ATR14
    tr = pd.concat([d['High']-d['Low'], (d['High']-d['Close'].shift()).abs(), (d['Low']-d['Close'].shift()).abs()], axis=1).max(axis=1)
    d['ATR14'] = tr.rolling(14, min_periods=1).mean()
    # RSI
    delta = d['Close'].diff()
    up = delta.clip(lower=0); down = -delta.clip(upper=0)
    ma_up = up.rolling(14, min_periods=1).mean(); ma_down = down.rolling(14, min_periods=1).mean()
    rs = ma_up / (ma_down + 1e-9)
    d['RSI'] = 100 - (100/(1+rs))
    # volume average
    d['Vol_MA20'] = d['Volume'].rolling(20, min_periods=1).mean()
    # range
    d['Range'] = (d['High'] - d['Low']) / (d['Low'] + 1e-9)
    # 52w H/L (optional, not used in default features but good to have)
    d['52w_high'] = d['Close'].rolling(252, min_periods=1).max()
    d['52w_low'] = d['Close'].rolling(252, min_periods=1).min()
    return d

# ---------- Candlestick patterns (simple) ----------
def detect_patterns(df):
    d = df.copy().reset_index(drop=True)
    d['Doji'] = 0; d['Hammer'] = 0; d['BullEng'] = 0; d['BearEng'] = 0
    for i in range(1,len(d)):
        o,c,h,l = d.at[i,'Open'], d.at[i,'Close'], d.at[i,'High'], d.at[i,'Low']
        po,pc = d.at[i-1,'Open'], d.at[i-1,'Close']
        body = abs(c-o); rng = h-l + 1e-9
        if body < 0.1*rng: d.at[i,'Doji'] = 1
        if body < 0.25*rng and (c-l) > 2*body: d.at[i,'Hammer'] = 1
        if (c>o) and (pc<po) and (c>=po) and (o<=pc): d.at[i,'BullEng'] = 1
        if (c<o) and (pc>po) and (o>=pc) and (c<=po): d.at[i,'BearEng'] = 1
    return d

# ---------- Prepare dataset (combined) ----------
def prepare_df(df, horizon, thresh_pct, features_list): # Pass horizon, threshold, and features list
    if df.empty:
        return pd.DataFrame(), []

    # Call add_indicators first
    df_with_indicators = add_indicators(df.copy()) # Work on a copy

    # Then call detect_patterns on the result
    df_combined = detect_patterns(df_with_indicators)

    # Calculate target AFTER adding indicators/patterns
    df_combined['Target'] = (((df_combined['Close'].shift(-horizon) / df_combined['Close']) - 1) >= thresh_pct).astype(int)

    # Drop rows where essential features or the target is missing
    # Only drop NaNs from features_list and 'Target' AFTER calculating Target
    # Ensure all features_list items and 'Target' are in the DataFrame before dropping
    cols_to_check = [col for col in features_list + ['Target'] if col in df_combined.columns]
    if len(cols_to_check) != len(features_list) + 1:
        missing_cols = [col for col in features_list + ['Target'] if col not in df_combined.columns]
        st.warning(f"⚠ Warning: Columns missing before dropna in prepare_df: {missing_cols}. These features will be excluded.")
        # Update features_list to only include available columns
        features_list = [f for f in features_list if f in df_combined.columns]
        cols_to_check = features_list + ['Target']

    # Ensure cols_to_check is not empty before dropping
    if not cols_to_check:
         st.warning("No valid columns to check after feature processing.")
         return pd.DataFrame(), []

    df_cleaned = df_combined.dropna(subset=cols_to_check).reset_index(drop=True)
    return df_cleaned, features_list # Return updated features_list


# small function for final mixed-signal rule
def tech_signal(row):
    # trend rules - safely access features using .get()
    sma50 = row.get('SMA50', np.nan); sma200 = row.get('SMA200', np.nan)
    ema20 = row.get('EMA20', np.nan); ema50 = row.get('EMA50', np.nan)
    rsi = row.get('RSI', np.nan); atr = row.get('ATR14', np.nan); close = row.get('Close', np.nan)
    atr_ratio = (atr / (close + 1e-9)) if not np.isnan(atr) and not np.isnan(close) and close != 0 else np.nan
    vol = row.get('Volume', np.nan); vol_ma = row.get('Vol_MA20', np.nan)

    # simple tech signals:
    trend_up = (sma50 > sma200) if not np.isnan(sma50) and not np.isnan(sma200) else False
    trend_down = (sma50 < sma200) if not np.isnan(sma50) and not np.isnan(sma200) else False
    strong_momentum = (ema20 > ema50) and trend_up if not np.isnan(ema20) and not np.isnan(ema50) else False
    overbought = rsi > 70 if not np.isnan(rsi) else False
    oversold = rsi < 30 if not np.isnan(rsi) else False
    vol_spike = vol > 1.5 * vol_ma if not np.isnan(vol) and not np.isnan(vol_ma) and vol_ma != 0 else False

    # rules
    if strong_momentum and not overbought and (not np.isnan(atr_ratio) and atr_ratio is not None and atr_ratio < 0.08):
        return 1
    if trend_down and overbought: # Using tech for sell too based on previous code
        return -1
    return 0

# Final blended signal
def blended_signal(row, prob, prob_buy, prob_sell): # Pass thresholds
    ml = 1 if prob >= prob_buy else (-1 if prob <= prob_sell else 0)
    tech = row.get('TechSignal', 0) # Safely get tech signal

    # strong agreement => take it
    if ml == 1 and tech == 1:
        return 2  # Strong BUY
    if ml == -1 and tech == -1:
        return -2 # Strong SELL
    if ml == 1 and tech != -1: # ML is BUY, tech is not SELL
        return 1  # BUY
    if ml == -1 and tech != 1: # ML is SELL, tech is not BUY
        return -1 # SELL
    # if ML neutral but tech strong
    if ml == 0 and tech == 1:
        return 1
    if ml == 0 and tech == -1:
        return -1
    return 0  # HOLD


# ensemble helpers (quiet)
@st.cache_resource # Cache model training
def train_models(X_train, y_train, features_list, random_state): # Removed default value here, relying on explicit passing
    # scale
    scaler = StandardScaler()
    # Ensure X_train is a DataFrame before fillingna and transforming
    if not isinstance(X_train, pd.DataFrame):
         X_train = pd.DataFrame(X_train, columns=features_list)
    Xs = scaler.fit_transform(X_train[features_list].fillna(0))

    pos = max(1, y_train.sum())
    neg = max(1, len(y_train)-pos)
    # Handle case where there are no positive or negative samples for scale_pos_weight
    scale_pos = neg / (pos + 1e-9)

    # XGBoost (quiet)
    xclf = xgb.XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.05,
                             use_label_encoder=False, eval_metric='logloss',
                             verbosity=0, random_state=random_state, n_jobs=-1, # Use passed random_state
                             scale_pos_weight=scale_pos)
    xclf.fit(Xs, y_train)
    cal_x = CalibratedClassifierCV(xclf, cv=3, method='sigmoid')
    cal_x.fit(Xs, y_train)
    # LightGBM (quiet)
    lclf = lgb.LGBMClassifier(n_estimators=200, max_depth=6, learning_rate=0.05,
                              subsample=0.8, verbose=-1, random_state=random_state, n_jobs=-1) # Use passed random_state
    lclf.fit(Xs, y_train)
    cal_l = CalibratedClassifierCV(lclf, cv=3, method='sigmoid')
    cal_l.fit(Xs, y_train)
    # RandomForest
    rclf = RandomForestClassifier(n_estimators=300, max_depth=8, class_weight='balanced', n_jobs=-1, random_state=random_state) # Use passed random_state
    rclf.fit(Xs, y_train)
    cal_r = CalibratedClassifierCV(rclf, cv=3, method='sigmoid')
    cal_r.fit(Xs, y_train)
    return scaler, [cal_x, cal_l, cal_r]

def ensemble_prob(X, scaler, calibrated_models, features_list):
    # Ensure X is a DataFrame before selecting columns and transforming
    if not isinstance(X, pd.DataFrame):
         X = pd.DataFrame(X, columns=features_list)
    Xs = scaler.transform(X[features_list].fillna(0))
    probs = np.vstack([c.predict_proba(Xs)[:,1] for c in calibrated_models])
    return probs.mean(axis=0)

# Backtester function
def backtest_signals_sl_tp(df_signals, signal_col='FinalSignal', stop_loss=0.03, take_profit=0.06, horizon=5, trade_fee=0.0005):
    """
    Simulates trading signals using stop-loss/take-profit and a fixed horizon.

    Args:
        df_signals (pd.DataFrame): DataFrame containing price data and trading signals.
                                   Must include 'Open', 'High', 'Low', 'Close', 'Date', and signal_col.
        signal_col (str): The name of the column containing the trading signals (1 or 2 for buy, -1 or -2 for sell, 0 for hold).
                           Assumes only BUY signals (> 0) are acted upon.
        stop_loss (float): Stop-loss percentage (e.g., 0.03 for 3%).
        take_profit (float): Take-profit percentage (e.g., 0.06 for 6%).
        horizon (int): Maximum number of days to hold a position if SL/TP are not hit.
        trade_fee (float): Commission per trade (applied on both entry and exit).

    Returns:
        tuple: A dictionary of backtesting statistics and a list of trade details.
    """
    df = df_signals.reset_index(drop=True).copy()
    trades = []
    open_positions = [] # Track open positions by entry index

    # Check if signal_col exists in the DataFrame before starting - Redundant with pre-check, but safe.
    if signal_col not in df.columns:
        st.error(f"Backtesting error: Signal column '{signal_col}' not found in the DataFrame.")
        return {'n_trades':0,'avg_return':0,'cum_return':0,'win_rate':0,'trades':[]}, []

    # Add a check for all NaN signals in the test set
    if df[signal_col].isnull().all():
         st.info(f"Backtesting skipped: '{signal_col}' column contains only missing values.")
         return {'n_trades':0,'avg_return':0,'cum_return':0,'win_rate':0,'trades':[]}, []

    # Get the integer index for the signal_col name once
    try:
        signal_col_idx = df.columns.get_loc(signal_col)
    except KeyError:
         st.error(f"Backtesting error: Signal column '{signal_col}' not found in DataFrame columns.")
         return {'n_trades':0,'avg_return':0,'cum_return':0,'win_rate':0,'trades':[]}, []


    for i in range(len(df) - 1): # Iterate up to the second to last day to allow entry on the next day
        # Check for signals from the previous day (signal on day i decides action on day i+1)
        # Use .iat for integer-based access and add check for valid signal value
        # Ensure index i is within bounds before accessing with .iat
        if i < len(df):
             # Access signal using .iat
             current_signal = df.iat[i, signal_col_idx]
        else:
             current_signal = np.nan # Treat out of bounds as NaN signal

        if pd.isna(current_signal) or current_signal <= 0: # Skip if signal is NaN or not a BUY signal
             continue

        if current_signal > 0: # BUY signal (1 or 2 for blended signal)
            entry_idx = i + 1
            if entry_idx >= len(df): break # Cannot enter on the last day

            # Check if there's an existing open position - simplified: only one position at a time
            if open_positions:
                 continue # Skip entry if already in a position


            entry_price = df.at[entry_idx, 'Open'] if not np.isnan(df.at[entry_idx, 'Open']) else df.at[entry_idx, 'Close']
            if np.isnan(entry_price) or entry_price <= 0: continue # Cannot enter if entry price is invalid

            # Record the open position
            open_positions.append({'entry_day': entry_idx, 'entry_price': entry_price, 'entry_date': df.at[entry_idx, 'Date']})


        # Check for exits (SL/TP or time horizon) for any open positions
        for pos in open_positions[:]: # Iterate over a copy to allow removal
             entry_day = pos['entry_day']
             entry_price = pos['entry_price']
             entry_date = pos['entry_date']

             exited = False
             exit_price = np.nan
             exit_day = np.nan

             # Simulate days from entry day + 1 up to horizon + buffer
             for j in range(entry_day + 1, min(entry_day + horizon + 5, len(df))): # Add a small buffer (e.g., +5 days) in case horizon day is holiday
                 if j >= len(df):
                     # Reached end of data before exit condition met
                     # Exit at the close of the last available day
                     if not np.isnan(df.at[len(df)-1, 'Close']):
                         exit_price = df.at[len(df)-1, 'Close']
                         exited = True
                         exit_day = len(df)-1
                     break # Exit loop for this position

                 high = df.at[j, 'High']; low = df.at[j, 'Low']; close = df.at[j, 'Close']

                 if np.isnan(high) or np.isnan(low) or np.isnan(close): continue # Skip if price data is missing for this day


                 # Check stop loss on Low price
                 if low <= entry_price * (1 - stop_loss):
                     # Assume exit happens at SL price if touched during the day
                     exit_price = entry_price * (1 - stop_loss)
                     exited = True
                     exit_day = j
                     break # Exit loop for this position

                 # Check take profit on High price
                 elif high >= entry_price * (1 + take_profit):
                     # Assume exit happens at TP price if touched during the day
                     exit_price = entry_price * (1 + take_profit)
                     exited = True
                     exit_day = j
                     break # Exit loop for this position

                 # Check horizon exit at the Close price on the horizon day
                 elif j == entry_day + horizon: # Exit at close on the day after the horizon period ends
                     if not np.isnan(close):
                         exit_price = close
                         exited = True
                         exit_day = j
                         break # Exit loop for this position


             if exited and not np.isnan(exit_price):
                 ret = (exit_price / entry_price) - 1
                 # Apply trade fees (entry and exit)
                 ret -= 2 * trade_fee if entry_price > 0 else 0 # Avoid division by zero
                 trades.append({'entry_date': df.at[entry_day,'Date'], 'exit_date': df.at[exit_day,'Date'],
                                'entry_price': entry_price, 'exit_price': exit_price, 'return': ret})
                 open_positions.remove(pos) # Remove this position

    # Summarize trades
    if len(trades) == 0:
        return {'n_trades':0,'avg_return':0,'cum_return':0,'win_rate':0,'trades':[]}, []

    rets = np.array([t['return'] for t in trades])
    cum_return = np.prod(1 + rets) - 1

    stats = {
        'n_trades': len(trades),
        'avg_return': float(np.mean(rets)) if len(rets) > 0 else 0,
        'cum_return': float(cum_return),
        'win_rate': float((rets > 0).sum() / len(rets)) if len(rets) > 0 else 0
    }
    return stats, trades

# ---------- Main Analysis Function (for a single ticker) ----------
@st.cache_data # Cache the results of the analysis pipeline for a given ticker and parameters
def run_analysis(ticker, start_date, end_date, horizon, thresh_pct, prob_buy, prob_sell, test_size, stop_loss, take_profit, trade_fee, random_state):
    # Explicitly set random_state inside the function using the passed argument
    # This is a workaround for the NameError issue the user is facing
    local_random_state = random_state

    results = {}

    try:
        st.subheader(f"Analyzing {ticker}")

        # 1. Fetch historical data
        df_hist = fetch_history_yf(ticker, start_date, end_date)
        if df_hist.empty:
            return {"status": "failed", "message": f"Could not fetch data for {ticker}"}
        results['data_range'] = f"{df_hist['Date'].min().date()} to {df_hist['Date'].max().date()} (rows: {len(df_hist)})"


        # 2. Prepare dataset
        FEATURES = ['SMA50','SMA200','EMA20','EMA50','MACD','ATR14','RSI','Vol_MA20','Range','Doji','Hammer','BullEng','BearEng','Close','Volume'] # Using features from the third notebook
        df, current_features = prepare_df(df_hist, horizon, thresh_pct, FEATURES)

        if df is None or df.empty:
            return {"status": "failed", "message": f"Data preparation failed or resulted in empty data for {ticker}"}
        results['usable_rows'] = df.shape[0]
        if len(current_features) != len(FEATURES):
             missing = [f for f in FEATURES if f not in current_features]
             st.warning(f"⚠ Warning: The following features were excluded for {ticker} due to missing data: {missing}.")
             FEATURES = current_features # Update FEATURES to the list actually used

        # 3. Train/Test Split (X and y for training)
        if len(df) < 200:
            st.warning(f"⚠ Not many rows available ({len(df)}) for {ticker}— consider increasing date range to improve model.")
            # Check if enough data for training after split
            if len(df) * (1 - test_size) < 50:
                 st.error(f"⚠ Critical: Insufficient data for training after split ({len(df) * (1 - test_size):.0f} rows) for {ticker}. Minimum 50 rows required for training. Skipping analysis.")
                 return {"status": "failed", "message": f"Insufficient data for training for {ticker}"}

        # Split for X and y *before* oversampling
        cutoff_idx = int(len(df) * (1 - test_size))
        X_train = df[FEATURES].iloc[:cutoff_idx].fillna(0)
        y_train = df['Target'].iloc[:cutoff_idx].astype(int)
        X_test = df[FEATURES].iloc[cutoff_idx:].fillna(0)
        y_test = df['Target'].iloc[cutoff_idx:].astype(int)

        # Pass test set length for metrics display
        results['test_set_rows'] = len(y_test)


        # 4. Balance training positives
        try:
            # Use local_random_state here
            ros = RandomOverSampler(random_state=local_random_state)
            X_res_np, y_res = ros.fit_resample(X_train.values, y_train.values)
            X_res = pd.DataFrame(X_res_np, columns=FEATURES)
            results['oversampling_info'] = f"Train before/after oversample: {len(X_train)}/{len(X_res)}  Positives after: {y_res.sum()}"
        except ImportError:
            st.warning("Imbalanced-learn not found. Skipping oversampling.")
            X_res = X_train.copy()
            y_res = y_train.copy()
        except Exception as e:
             st.error(f"Error during oversampling for {ticker}: {e}")
             X_res = X_train.copy()
             y_res = y_train.copy()
             st.warning("Proceeding without oversampling.")

        # 5. Train ensemble
        # Pass X_res and y_res (potentially oversampled) to train_models
        scaler, calibrated_models = train_models(X_res, y_res, FEATURES, random_state=local_random_state)
        st.success("Models trained and calibrated.")

        # 6. Ensemble predict on full df
        X_full = df[FEATURES].fillna(0)
        probs_full = ensemble_prob(X_full, scaler, calibrated_models, FEATURES)
        df['PredProb'] = probs_full
        df['ML_Pred'] = (df['PredProb'] >= prob_buy).astype(int)

        # 7. Add Technical and Blended Signals
        df['TechSignal'] = df.apply(tech_signal, axis=1)
        df['FinalSignal'] = df.apply(lambda r: blended_signal(r, r['PredProb'], prob_buy, prob_sell), axis=1)

        # 8. Model Performance (on Test Set)
        # Ensure X_test and y_test are derived from the original df split, not the df modified with signals
        # Calculate metrics on the test split's actuals vs. predictions from the model trained on (potentially oversampled) train data
        probs_test = ensemble_prob(X_test, scaler, calibrated_models, FEATURES)
        preds_test = (probs_test >= prob_buy).astype(int)


        try:
            # Check if y_test is not empty and has more than one class before computing metrics
            if len(np.unique(y_test)) > 1:
                auc = roc_auc_score(y_test, probs_test)
                report = classification_report(y_test, preds_test, output_dict=True, zero_division=0)
                acc = report['accuracy']
                prec = report['macro avg']['precision']
                rec = report['macro avg']['recall']
                f1 = report['macro avg']['f1-score']
                results['model_metrics'] = {
                    'Test Rows': results.get('test_set_rows', 'N/A'), # Use the saved test set length
                    'Accuracy': f"{acc:.3f}",
                    'Macro-Precision': f"{prec:.3f}",
                    'Macro-Recall': f"{rec:.3f}",
                    'Macro-F1': f"{f1:.3f}",
                    'ROC-AUC': f"{auc:.3f}"
                }
            else:
                 st.warning(f"Could not compute full model metrics for {ticker}: Test set contains only one class.")
                 results['model_metrics'] = {'Message': 'Test set contains only one class. Cannot compute full metrics.'}

        except Exception as e:
             st.warning(f"Could not compute full model metrics for {ticker}: {e}")
             results['model_metrics'] = {'Message': f'Error computing metrics: {e}'}


        # 9. Latest Signal and Recommendation
        # Use the full df *after* signals are added for the latest signal
        latest = df.iloc[-1]
        signal_map = {2:"STRONG BUY",1:"BUY",0:"HOLD",-1:"SELL",-2:"STRONG SELL"}
        final_txt = signal_map.get(latest.get('FinalSignal', 0), 'HOLD') # Safely get signal, default to 0
        prob_latest = latest.get('PredProb', np.nan) # Safely get probability

        results['latest_signal'] = {
            'Date': latest['Date'].date(),
            'Close': f"{latest['Close']:.2f}",
            'Prob(BUY)': f"{prob_latest:.3f}" if not np.isnan(prob_latest) else 'N/A',
            'Final Recommendation': final_txt
        }


        # 10. Backtesting
        st.subheader(f"Backtesting Results for {ticker} (Test Set)")
        # Pass the test split of df *after* signals have been added
        test_with_signals = df.iloc[cutoff_idx:].copy().reset_index(drop=True)

        # Add checks for test_with_signals before backtesting
        if test_with_signals.empty:
            st.info(f"Backtesting skipped for {ticker}: Test set is empty.")
            results['backtest_stats'] = {'Message': "Backtesting skipped: Test set is empty."}
        elif 'FinalSignal' not in test_with_signals.columns:
             st.error(f"Backtesting preparation failed for {ticker}: 'FinalSignal' column is missing in the test set.")
             results['backtest_stats'] = {'Message': "Backtesting failed: 'FinalSignal' column missing in test set."}
        elif test_with_signals['FinalSignal'].isnull().all():
             st.info(f"Backtesting skipped for {ticker}: 'FinalSignal' column in test set contains only missing values.")
             results['backtest_stats'] = {'Message': "Backtesting skipped: 'FinalSignal' column contains only missing values."}
        else:
             # Add debug print for columns and non-null count
             # print(f"DEBUG: Columns in test_with_signals for {ticker}: {test_with_signals.columns.tolist()}") # Removed debug print
             # print(f"DEBUG: Non-null count in FinalSignal column: {test_with_signals['FinalSignal'].notnull().sum()} / {len(test_with_signals)}") # Removed debug print

             bt_stats, bt_trades = backtest_signals_sl_tp(test_with_signals, signal_col='FinalSignal',
                                                           stop_loss=stop_loss, take_profit=take_profit,
                                                           horizon=horizon, trade_fee=trade_fee)

             results['backtest_stats'] = bt_stats
             results['backtest_trades'] = bt_trades # Keep trades for potential display

             if bt_stats and bt_stats['n_trades'] > 0:
                  bt_stats_df = pd.DataFrame([bt_stats]).T.rename(columns={0:'Value'})
                  if 'trades' in bt_stats_df.index:
                      bt_stats_df = bt_stats_df.drop('trades')
                  st.dataframe(bt_stats_df.style.format({'Value': '{:.4f}'}))
             elif bt_stats and bt_stats['n_trades'] == 0:
                st.info("No trades were generated in the backtest on the test set.")


        # 11. Plotting
        st.subheader(f"Price Chart and Signals for {ticker}")
        plot_n = 200 # Fixed number of days for plot
        plot_df = df.tail(plot_n).copy() # Use full df for plotting signals
        fig = go.Figure()
        fig.add_trace(go.Candlestick(x=plot_df['Date'], open=plot_df['Open'], high=plot_df['High'],
                                     low=plot_df['Low'], close=plot_df['Close'], name="OHLC"))
        for tech_plot_feat in ['SMA50', 'SMA200']:
            if tech_plot_feat in plot_df.columns:
                 color = 'blue' if tech_plot_feat == 'SMA50' else 'orange'
                 fig.add_trace(go.Scatter(x=plot_df['Date'], y=plot_df[tech_plot_feat], name=tech_plot_feat, line=dict(color=color)))

        # Ensure 'FinalSignal' is in plot_df before filtering
        if 'FinalSignal' in plot_df.columns:
             buys = plot_df[plot_df['FinalSignal']>0]
             sells = plot_df[plot_df['FinalSignal']<0]
             if not buys.empty:
                 fig.add_trace(go.Scatter(x=buys['Date'], y=buys['Close'], mode='markers', marker_symbol='triangle-up', marker_color='green', marker_size=10, name='Buy Signal'))
             if not sells.empty:
                 fig.add_trace(go.Scatter(x=sells['Date'], y=sells['Close'], mode='markers', marker_symbol='triangle-down', marker_color='red', marker_size=10, name='Sell Signal'))

        fig.update_layout(title=f"{ticker} - last {plot_n} days with Signals", xaxis_rangeslider_visible=False, height=500)
        st.plotly_chart(fig, use_container_width=True)

        # 12. SHAP Analysis for latest BUY signal
        st.subheader(f"SHAP Analysis for Latest BUY Signal ({ticker})")
        try:
            # Ensure df has the 'FinalSignal' column before attempting SHAP
            if 'FinalSignal' in df.columns:
                # Find the latest BUY signal row *after* signals are added
                latest_buy_signal_row = df[df['FinalSignal'] > 0].tail(1)
            else:
                latest_buy_signal_row = pd.DataFrame() # Empty DataFrame if column doesn't exist


            if not latest_buy_signal_row.empty:
                # Ensure we have the scaler and trained models from the training step
                # Ensure FEATURES list is not empty
                if scaler and calibrated_models and FEATURES and len(FEATURES) > 0:
                     shap_data = latest_buy_signal_row[FEATURES].fillna(0)
                     # Check if shap_data is empty or has invalid shape before scaling
                     if shap_data.empty or shap_data.shape[1] != len(FEATURES):
                         st.warning("SHAP input data is empty or has incorrect features. Cannot generate waterfall plot.")
                         shap_values = None
                     else:
                          shap_data_scaled = scaler.transform(shap_data)


                          # Use X_train (before oversampling) for SHAP background
                          # Use local_random_state here
                          # Ensure X_train is not empty and has correct features before sampling
                          if X_train.empty or X_train.shape[1] != len(FEATURES):
                               st.warning("SHAP background data (X_train) is empty or has incorrect features. Cannot generate waterfall plot.")
                               background_data = None
                          else:
                               background_data = scaler.transform(X_train[FEATURES].fillna(0).sample(min(100, len(X_train)), random_state=local_random_state))


                          if background_data is not None:
                               explainer = shap.Explainer(calibrated_models[0].predict_proba, background_data, feature_names=FEATURES)
                               # Ensure shap_data_scaled has the correct shape (1, num_features)
                               if shap_data_scaled.shape[0] == 1 and shap_data_scaled.shape[1] == len(FEATURES):
                                    shap_values = explainer(shap_data_scaled[0, :].reshape(1, -1))
                               else:
                                    st.warning("SHAP input data has unexpected shape after scaling. Cannot generate waterfall plot.")
                                    shap_values = None
                          else:
                              st.warning("SHAP background data is not available. Cannot generate waterfall plot.")
                              shap_values = None


                     st.write("Top 5 feature contributions for the latest BUY signal:")

                     if shap_values is not None:
                         # Check dimensions and output_dims before indexing
                         # Removed feature_names argument from waterfall plot
                         if hasattr(shap_values, 'values') and shap_values.values.ndim == 3 and shap_values.values.shape[2] > 1:
                              # Assuming multi-output model (predict_proba returns class probabilities)
                              # Index for positive class (1)
                              shap_values_instance = shap_values[0, :, 1]
                         elif hasattr(shap_values, 'values') and shap_values.values.ndim == 2 and getattr(shap_values, 'output_dims', 0) == 1:
                              # Assuming single output model (e.g., predict_proba returns probability of class 1 directly)
                              # Values are already 2D (instance, features)
                              shap_values_instance = shap_values[0, :]
                         else:
                              st.warning(f"Unexpected SHAP values format (ndim={getattr(shap_values, 'values', None).ndim if hasattr(shap_values, 'values') else 'N/A'}, output_dims={getattr(shap_values, 'output_dims', 'N/A')}). Cannot generate waterfall plot.")
                              shap_values_instance = None

                         if shap_values_instance is not None:
                             # Use shap.waterfall plot with matplotlib backend for Streamlit
                             fig_mpl, ax = plt.subplots(figsize=(8, 6))
                             # Ensure feature_names are correctly passed and match the length of shap_values_instance
                             # Removed feature_names argument here
                             shap.plots.waterfall(shap_values_instance, max_display=5, show=False)
                             plt.title(f"SHAP Waterfall Plot for {ticker}")
                             st.pyplot(fig_mpl) # Display plot in Streamlit
                             plt.close(fig_mpl) # Close figure
                             # else: # This else block is now redundant as feature_names is removed from waterfall call
                             #      st.warning("Mismatch between SHAP values length and feature names length. Cannot generate waterfall plot.")
                             #      plt.close(fig_mpl) # Close figure to prevent display issues

                         else:
                             st.info("Could not prepare SHAP values instance for plotting.")


                else:
                     st.warning("Scaler, trained models, or features not available for SHAP analysis.")

            else:
                st.info("No recent BUY signals found for SHAP analysis with current parameters for this ticker.")

        except ImportError:
            st.warning("SHAP library not found. Install with `pip install shap` to enable SHAP analysis.")
        except Exception as e:
            st.error(f"SHAP analysis error for {ticker}: {e}")


        results['status'] = "success"
        results['df'] = df # Return the full dataframe for potential further use
        return results

    except Exception as e:
        st.error(f"An unexpected error occurred during analysis for {ticker}: {e}")
        return {"status": "failed", "message": f"An unexpected error occurred for {ticker}: {e}"}


# ---------- Streamlit App Layout and Logic ----------
st.set_page_config(layout="wide", page_title="NSE Stock Analysis App")

st.title("NSE Stock Analysis with ML & Technical Signals")

# --- Sidebar for User Inputs ---
st.sidebar.header("Analysis Parameters")

# Dynamic list of Nifty 500 tickers
all_tickers = get_nifty500_tickers()
default_tickers = ["RELIANCE", "TCS"] # Default selection

selected_tickers = st.sidebar.multiselect(
    "Select NSE Stock(s):",
    options=all_tickers,
    default=default_tickers
)

st.sidebar.subheader("Date Range")
today = datetime.today()
end_date = st.sidebar.date_input("End Date:", value=today)
start_date = st.sidebar.date_input("Start Date:", value=today - timedelta(days=365*5))

# Ensure start_date is before end_date
if start_date and end_date and start_date >= end_date:
    st.sidebar.error("Error: Start date must be before end date.")

# Define default values for sliders
default_horizon = 5
default_thresh_pct = 0.05
default_prob_buy = 0.55
default_prob_sell = 0.45
default_test_size = 0.2
default_stop_loss = 0.03
default_take_profit = 0.06
default_trade_fee = 0.0005


st.sidebar.subheader("Model & Signal Parameters")
horizon = st.sidebar.slider("Prediction Horizon (Days Ahead):", 1, 20, default_horizon)
thresh_pct = st.sidebar.slider("Target Threshold (% Price Increase):", 0.01, 0.10, default_thresh_pct, 0.005)
prob_buy = st.sidebar.slider("ML Probability Threshold (BUY):", 0.5, 1.0, default_prob_buy, 0.01)
prob_sell = st.sidebar.slider("ML Probability Threshold (SELL):", 0.0, 0.5, default_prob_sell, 0.01)
test_size = st.sidebar.slider("Test Set Size (%):", 0.1, 0.5, default_test_size, 0.05)

st.sidebar.subheader("Backtesting Parameters")
stop_loss = st.sidebar.slider("Stop Loss (%):", 0.01, 0.10, default_stop_loss, 0.005)
take_profit = st.sidebar.slider("Take Profit (%):", 0.01, 0.20, default_take_profit, 0.005)
trade_fee = st.sidebar.slider("Trade Fee (% per trade):", 0.0, 0.01, default_trade_fee, 0.0001)


# --- Main Content Area ---
# Only proceed if tickers are selected and dates are valid
if selected_tickers and start_date and end_date and start_date < end_date:
    st.header("Analysis Results")

    for ticker in selected_tickers:
        st.markdown("---") # Separator for each ticker
        # Run the analysis pipeline for the selected ticker
        # The NameError was occurring here - ensure RANDOM_STATE is accessible
        # Explicitly passing RANDOM_STATE to run_analysis which now has a dedicated local_random_state
        analysis_results = run_analysis(
            ticker,
            start_date.strftime("%Y-%m-%d"),
            end_date.strftime("%Y-%m-%d"),
            horizon,
            thresh_pct,
            prob_buy,
            prob_sell,
            test_size,
            stop_loss,
            take_profit,
            trade_fee,
            RANDOM_STATE # Pass the global variable
        )

        if analysis_results.get("status") == "success":
            st.subheader(f"Summary for {ticker}")

            col1, col2 = st.columns(2)
            with col1:
                st.write("**Data Range:**", analysis_results.get('data_range', 'N/A'))
                st.write("**Usable Rows (after features):**", analysis_results.get('usable_rows', 'N/A'))
                if 'oversampling_info' in analysis_results:
                    st.write("**Oversampling Info:**", analysis_results['oversampling_info'])


            with col2:
                 st.write("**Latest Signal:**")
                 latest_sig_data = analysis_results.get('latest_signal', {})
                 if latest_sig_data:
                     st.write(f"**Date:** {latest_sig_data.get('Date', 'N/A')}")
                     st.write(f"**Close:** {latest_sig_data.get('Close', 'N/A')}")
                     st.write(f"**Prob(BUY):** {latest_sig_data.get('Prob(BUY)', 'N/A')}")
                     final_rec = latest_sig_data.get('Final Recommendation', 'HOLD')
                     color = "green" if "BUY" in final_rec else ("red" if "SELL" in final_rec else "orange")
                     st.markdown(f"**Final Recommendation:** <span style='color:{color}'>**{final_rec}**</span>", unsafe_allow_html=True)


            st.subheader("Model Metrics (Test Set)")
            metrics = analysis_results.get('model_metrics', {})
            if metrics:
                if 'Message' in metrics:
                    st.info(metrics['Message'])
                else:
                    metrics_df = pd.DataFrame([metrics]).T.rename(columns={0:'Value'})
                    st.dataframe(metrics_df)

            st.subheader("Backtest Statistics (Test Set)")
            bt_stats = analysis_results.get('backtest_stats', {})
            if bt_stats and 'Message' in bt_stats:
                st.info(bt_stats['Message']) # Display message if backtesting was skipped or had an issue
            elif bt_stats and bt_stats['n_trades'] > 0:
                 bt_stats_df = pd.DataFrame([bt_stats]).T.rename(columns={0:'Value'})
                 # Remove the 'trades' list from display if it exists in stats
                 if 'trades' in bt_stats_df.index:
                     bt_stats_df = bt_stats_df.drop('trades')
                 st.dataframe(bt_stats_df.style.format({'Value': '{:.4f}'}))

                 # Display sample trades
                 # st.write("Sample Trades:") # Optionally display sample trades
                 # sample_trades = analysis_results.get('backtest_trades', [])[:5] # Limit to first 5
                 # if sample_trades:
                 #      sample_trades_df = pd.DataFrame(sample_trades)
                 #      st.dataframe(sample_trades_df.style.format({'entry_price':'{:.2f}', 'exit_price':'{:.2f}', 'return':'{:.4f}'}))
                 # else:
                 #      st.info("No sample trades to display.")

            elif bt_stats and bt_stats['n_trades'] == 0:
                st.info("No trades were generated in the backtest on the test set.")
            # Removed redundant else None check


        elif analysis_results.get("status") == "failed":
            st.error(f"Analysis failed for {ticker}: {analysis_results.get('message', 'Unknown error')}")

else:
    st.info("Please select at least one stock and ensure the start date is before the end date in the sidebar.")

st.markdown("---")
st.markdown("Built with Streamlit, yfinance, scikit-learn, xgboost, lightgbm, imbalanced-learn, shap, plotly, pandas, numpy.")